{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "l8s8rVa8iGfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download('punkt') #tokenizer, run once\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypqof9KLlmQ9",
        "outputId": "c695859b-caca-4b2d-bb7f-4f7137e2658e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notes=pd.read_csv('Final_SWmerged.csv')"
      ],
      "metadata": {
        "id": "JgEpO8-Li7uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Processing**"
      ],
      "metadata": {
        "id": "5fxVqX8-jNN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notes[\"TEXT\"] = [note.lower() for note in notes[\"TEXT\"]]"
      ],
      "metadata": {
        "id": "4HVeVlrujCjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(df):\n",
        "  text = str(df[\"TEXT\"])\n",
        "  clean = re.sub(r\"\\n\",\"\",text)\n",
        "  cleaner = re.sub(r\"  \",\"\",clean)\n",
        "\n",
        "  return cleaner"
      ],
      "metadata": {
        "id": "Rzk0KfA9jDai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes[\"TEXT_CLEAN\"] = notes.apply(clean_text, axis=1)"
      ],
      "metadata": {
        "id": "Drfksa6puo1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean = notes.apply(clean_text, axis=1)"
      ],
      "metadata": {
        "id": "roMGrC3ljIu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize words and remove punctuation\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
      ],
      "metadata": {
        "id": "SWXZmA22oJTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw_words = list(sent_to_words(text_clean))"
      ],
      "metadata": {
        "id": "zVuR-fjVoXt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building bigrams and trigrams\n",
        "bigram = gensim.models.Phrases(sw_words, min_count=5, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[sw_words], threshold=100)\n",
        "\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv5rh8VdouDD",
        "outputId": "b07c18ac-ec8f-4e60-a5a6-b5be7e1b1ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions for creating bigrams and trimagrams in documents\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
      ],
      "metadata": {
        "id": "iBXHNplJpCMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make bigrams and trigrams\n",
        "sw_words_bigrams = make_bigrams(sw_words)\n",
        "\n",
        "sw_words_trigrams = make_trigrams(sw_words_bigrams)"
      ],
      "metadata": {
        "id": "LaNpAOgls2XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## W2V Model"
      ],
      "metadata": {
        "id": "N9XGiY6juHAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = gensim.models.Word2Vec(\n",
        "        sw_words_trigrams,\n",
        "        size=100,\n",
        "        window=5,\n",
        "        min_count=5,\n",
        "        workers=10,\n",
        "        sg=1)"
      ],
      "metadata": {
        "id": "fwELJwgitXTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sw_words_trigrams,total_examples=len(sw_words_trigrams),epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fUscdKduKen",
        "outputId": "ddf52dd0-4798-4f91-d973-13205d08b2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2395472, 3553050)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking avg length of notes\n",
        "\n",
        "def text_splitter(df):\n",
        "    text = df[\"TEXT_CLEAN\"]\n",
        "    content = text.split()\n",
        "    return content\n",
        "\n",
        "def avg_doc_length(corp):\n",
        "    total = 0\n",
        "    for doc in corp:\n",
        "        total += len(doc)\n",
        "    print (total)\n",
        "    avg = total/len(corp)\n",
        "    return avg"
      ],
      "metadata": {
        "id": "gAeSkZ3QuYEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Notes into Embeddings"
      ],
      "metadata": {
        "id": "F-cTDpCnySNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = w2v_model.wv"
      ],
      "metadata": {
        "id": "VAuaz4fi3_mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc"
      ],
      "metadata": {
        "id": "04LN6_5nyRMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [preprocess(text) for text in text_clean]\n",
        "labels = notes.iloc[:,11:15]"
      ],
      "metadata": {
        "id": "DxI1HNl90zB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering documents to make sure at least one word in a note has a vector representation"
      ],
      "metadata": {
        "id": "veikWjyq7ncs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_docs(corpus, texts, condition_on_doc):\n",
        "    \"\"\"\n",
        "    Filter corpus, texts and labels given the function condition_on_doc which takes\n",
        "    a doc.\n",
        "    The document doc is kept if condition_on_doc(doc) is true.\n",
        "    \"\"\"\n",
        "    if texts is not None:\n",
        "        texts = [text for (text, doc) in zip(texts, corpus)\n",
        "                 if condition_on_doc(doc)]\n",
        "\n",
        "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
        "\n",
        "    return (corpus, texts)"
      ],
      "metadata": {
        "id": "uQs47ZG528_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Condition used for function above\n",
        "def has_vector_representation(word2vec_model, doc):\n",
        "    \"\"\"check if at least one word of the document is in the\n",
        "    word2vec dictionary\"\"\"\n",
        "    return not all(word not in word2vec_model.vocab for word in doc)"
      ],
      "metadata": {
        "id": "5JsHrR3J2-WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus, text_clean  = filter_docs(corpus, text_clean, lambda doc: has_vector_representation(word_vectors, doc))"
      ],
      "metadata": {
        "id": "G3vgEh9C3MER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_vector(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
        "    #Average the vectors of each word in the note\n",
        "    return np.mean(word2vec_model[doc], axis=0)"
      ],
      "metadata": {
        "id": "0ciDeaid6abH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embedding =[]\n",
        "for doc in corpus:\n",
        "    doc_embedding.append(document_vector(word_vectors, doc))"
      ],
      "metadata": {
        "id": "9g8SVD9C4wOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "note_w2v = np.array(doc_embedding)"
      ],
      "metadata": {
        "id": "fs-okhqe4zg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Classifier"
      ],
      "metadata": {
        "id": "c7Vdp5h68KYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert y to np arrays\n",
        "label_array = np.array(labels)"
      ],
      "metadata": {
        "id": "kLMLuQpA8Rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LR_trainer(X_trn,y_trn,l):\n",
        "\n",
        "  p_avgs = []\n",
        "  r_avgs = []\n",
        "  f1_avgs = []\n",
        "\n",
        "  p_idvs = []\n",
        "  r_idvs = []\n",
        "  f1_idvs = []\n",
        "\n",
        "  lr = LogisticRegression(penalty='none',C=1/l) #solver='saga',l1_ratio=0.5\n",
        "  mt_lr = MultiOutputClassifier(lr, n_jobs=1)\n",
        "\n",
        "  kf = KFold(n_splits=5, shuffle=True,random_state= 1)\n",
        "\n",
        "  for train_index, test_index in kf.split(X_trn):\n",
        "    X_train, X_test = X_trn[train_index], X_trn[test_index]\n",
        "    y_train, y_test = y_trn[train_index], y_trn[test_index]\n",
        "\n",
        "    fit_model = mt_lr.fit(X_train,y_train)\n",
        "    y_pred = fit_model.predict(X_test)\n",
        "\n",
        "    #Precision, recall, f-score\n",
        "\n",
        "    p_avg = precision_score(y_test, y_pred, average='macro')\n",
        "    r_avg = recall_score(y_test, y_pred, average='macro')\n",
        "    f_avg = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    p_idv = precision_score(y_test, y_pred, average=None)\n",
        "    r_idv = recall_score(y_test, y_pred, average=None)\n",
        "    f_idv = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "    p_avgs.append(p_avg)\n",
        "    r_avgs.append(r_avg)\n",
        "    f1_avgs.append(f_avg)\n",
        "\n",
        "    p_idvs.append(p_idv)\n",
        "    r_idvs.append(r_idv)\n",
        "    f1_idvs.append(f_idv)\n",
        "\n",
        "  p_avg_cv = sum(p_avgs)/5\n",
        "  r_avg_cv = sum(r_avgs)/5\n",
        "  f1_avg_cv = sum(f1_avgs)/5\n",
        "\n",
        "  p_std = np.std(p_avgs)\n",
        "  r_std = np.std(r_avgs)\n",
        "  f1_std = np.std(f1_avgs)\n",
        "\n",
        "  p_idv_cv = sum(p_idvs)/5\n",
        "  r_idv_cv = sum(r_idvs)/5\n",
        "  f1_idv_cv = sum(f1_idvs)/5\n",
        "\n",
        "  p_idv_std = np.std(p_idvs,axis=0)\n",
        "  r_idv_std = np.std(r_idvs,axis=0)\n",
        "  f1_idv_std = np.std(f1_idvs,axis=0)\n",
        "\n",
        "  #Calculating CIs\n",
        "\n",
        "  p_ci = []\n",
        "  r_ci = []\n",
        "  f1_ci = []\n",
        "\n",
        "  p_ci_lb = p_avg_cv - (1.96*p_std)\n",
        "  p_ci_ub = p_avg_cv + (1.96*p_std)\n",
        "  p_ci.append(p_ci_lb)\n",
        "  p_ci.append(p_ci_ub)\n",
        "\n",
        "  r_ci_lb = r_avg_cv - (1.96*r_std)\n",
        "  r_ci_ub = r_avg_cv + (1.96*r_std)\n",
        "  r_ci.append(r_ci_lb)\n",
        "  r_ci.append(r_ci_ub)\n",
        "\n",
        "  f1_ci_lb = f1_avg_cv - (1.96*f1_std)\n",
        "  f1_ci_ub = f1_avg_cv + (1.96*f1_std)\n",
        "  f1_ci.append(f1_ci_lb)\n",
        "  f1_ci.append(f1_ci_ub)\n",
        "\n",
        "  #CIs for each category\n",
        "\n",
        "  #Precision\n",
        "\n",
        "  p_ci_ES = []\n",
        "  p_ci_CF = []\n",
        "  p_ci_C = []\n",
        "  p_ci_PA = []\n",
        "\n",
        "  p_ES_lb = p_idv_cv[0] - (1.96*p_idv_std[0])\n",
        "  p_ES_ub = p_idv_cv[0] + (1.96*p_idv_std[0])\n",
        "  p_ci_ES.append(p_ES_lb)\n",
        "  p_ci_ES.append(p_ES_ub)\n",
        "\n",
        "  p_CF_lb = p_idv_cv[1] - (1.96*p_idv_std[1])\n",
        "  p_CF_ub = p_idv_cv[1] + (1.96*p_idv_std[1])\n",
        "  p_ci_CF.append(p_CF_lb)\n",
        "  p_ci_CF.append(p_CF_ub)\n",
        "\n",
        "  p_C_lb = p_idv_cv[2] - (1.96*p_idv_std[2])\n",
        "  p_C_ub = p_idv_cv[2] + (1.96*p_idv_std[2])\n",
        "  p_ci_C.append(p_C_lb)\n",
        "  p_ci_C.append(p_C_ub)\n",
        "\n",
        "  p_PA_lb = p_idv_cv[3] - (1.96*p_idv_std[3])\n",
        "  p_PA_ub = p_idv_cv[3] + (1.96*p_idv_std[3])\n",
        "  p_ci_PA.append(p_PA_lb)\n",
        "  p_ci_PA.append(p_PA_ub)\n",
        "\n",
        "  #Recall\n",
        "\n",
        "  r_ci_ES = []\n",
        "  r_ci_CF = []\n",
        "  r_ci_C = []\n",
        "  r_ci_PA = []\n",
        "\n",
        "  r_ES_lb = r_idv_cv[0] - (1.96*r_idv_std[0])\n",
        "  r_ES_ub = r_idv_cv[0] + (1.96*r_idv_std[0])\n",
        "  r_ci_ES.append(r_ES_lb)\n",
        "  r_ci_ES.append(r_ES_ub)\n",
        "\n",
        "  r_CF_lb = r_idv_cv[1] - (1.96*r_idv_std[1])\n",
        "  r_CF_ub = r_idv_cv[1] + (1.96*r_idv_std[1])\n",
        "  r_ci_CF.append(r_CF_lb)\n",
        "  r_ci_CF.append(r_CF_ub)\n",
        "\n",
        "  r_C_lb = r_idv_cv[2] - (1.96*r_idv_std[2])\n",
        "  r_C_ub = r_idv_cv[2] + (1.96*r_idv_std[2])\n",
        "  r_ci_C.append(r_C_lb)\n",
        "  r_ci_C.append(r_C_ub)\n",
        "\n",
        "  r_PA_lb = r_idv_cv[3] - (1.96*r_idv_std[3])\n",
        "  r_PA_ub = r_idv_cv[3] + (1.96*r_idv_std[3])\n",
        "  r_ci_PA.append(r_PA_lb)\n",
        "  r_ci_PA.append(r_PA_ub)\n",
        "\n",
        "  #F1\n",
        "\n",
        "  f1_ci_ES = []\n",
        "  f1_ci_CF = []\n",
        "  f1_ci_C = []\n",
        "  f1_ci_PA = []\n",
        "\n",
        "  f1_ES_lb = f1_idv_cv[0] - (1.96*f1_idv_std[0])\n",
        "  f1_ES_ub = f1_idv_cv[0] + (1.96*f1_idv_std[0])\n",
        "  f1_ci_ES.append(f1_ES_lb)\n",
        "  f1_ci_ES.append(f1_ES_ub)\n",
        "\n",
        "  f1_CF_lb = f1_idv_cv[1] - (1.96*f1_idv_std[1])\n",
        "  f1_CF_ub = f1_idv_cv[1] + (1.96*f1_idv_std[1])\n",
        "  f1_ci_CF.append(f1_CF_lb)\n",
        "  f1_ci_CF.append(f1_CF_ub)\n",
        "\n",
        "  f1_C_lb = f1_idv_cv[2] - (1.96*f1_idv_std[2])\n",
        "  f1_C_ub = f1_idv_cv[2] + (1.96*f1_idv_std[2])\n",
        "  f1_ci_C.append(f1_C_lb)\n",
        "  f1_ci_C.append(f1_C_ub)\n",
        "\n",
        "  f1_PA_lb = f1_idv_cv[3] - (1.96*f1_idv_std[3])\n",
        "  f1_PA_ub = f1_idv_cv[3] + (1.96*f1_idv_std[3])\n",
        "  f1_ci_PA.append(f1_PA_lb)\n",
        "  f1_ci_PA.append(f1_PA_ub)\n",
        "\n",
        "  return p_idv_cv,p_avg_cv,r_idv_cv,r_avg_cv,f1_idv_cv,f1_avg_cv,p_ci_ES,p_ci_CF,p_ci_C,p_ci_PA,p_ci,r_ci_ES,r_ci_CF,r_ci_C,r_ci_PA,r_ci,f1_ci_ES,f1_ci_CF,f1_ci_C,f1_ci_PA,f1_ci"
      ],
      "metadata": {
        "id": "Vrh8zzSU6EsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_trainer(note_w2v,label_array,1) #No regularization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9iUcvUv6RNc",
        "outputId": "32a57fc6-cb33-4471-dcb1-44cc326cf8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.74462592, 0.52129926, 0.35245098, 0.57458272]),\n",
              " 0.5482397203297614,\n",
              " array([0.70279152, 0.37493673, 0.17383227, 0.40182191]),\n",
              " 0.41334560849693247,\n",
              " array([0.72227498, 0.4329697 , 0.23107938, 0.47201988]),\n",
              " 0.4645859852912982,\n",
              " [0.6878486914914329, 0.8014031525549318],\n",
              " [0.3500493785117351, 0.6925491347813966],\n",
              " [0.14976419847290395, 0.5551377623114098],\n",
              " [0.3994045829137326, 0.7497608616005492],\n",
              " [0.4469607255192356, 0.6495187151402873],\n",
              " [0.6508195563501222, 0.7547634909911567],\n",
              " [0.2335951921582677, 0.5162782757553736],\n",
              " [0.09867837430049238, 0.24898616321885458],\n",
              " [0.2776001851661295, 0.5260436300350628],\n",
              " [0.343562272006384, 0.48312894498748093],\n",
              " [0.6962720081454788, 0.7482779511141066],\n",
              " [0.29928550807070997, 0.566653885868684],\n",
              " [0.12036218937818266, 0.3417965699991685],\n",
              " [0.3322053609841673, 0.6118344087698881],\n",
              " [0.3843319312660298, 0.5448400393165665])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}